{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b5d710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\gvoff\\appdata\\roaming\\python\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting python-docx\n",
      "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/3e/3d/330d9efbdb816d3f60bf2ad92f05e1708e4a1b9abe80461ac3444c83f749/python_docx-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from python-docx) (4.9.0)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/244.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 30.7/244.3 kB 1.4 MB/s eta 0:00:01\n",
      "   --------- ----------------------------- 61.4/244.3 kB 365.7 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 92.2/244.3 kB 585.1 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 194.6/244.3 kB 787.7 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 194.6/244.3 kB 787.7 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 204.8/244.3 kB 655.1 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 204.8/244.3 kB 655.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 244.3/244.3 kB 576.5 kB/s eta 0:00:00\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1134646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx\n",
      "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
      "     ---------------------------------------- 0.0/54.9 kB ? eta -:--:--\n",
      "     -------------- ----------------------- 20.5/54.9 kB 320.0 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 51.2/54.9 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 54.9/54.9 kB 572.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from docx) (4.9.3)\n",
      "Requirement already satisfied: Pillow>=2.0 in c:\\users\\gvoff\\anaconda3\\lib\\site-packages (from docx) (9.4.0)\n",
      "Building wheels for collected packages: docx\n",
      "  Building wheel for docx (setup.py): started\n",
      "  Building wheel for docx (setup.py): finished with status 'done'\n",
      "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53905 sha256=81a76cc18b23048c77ad3bd360c4250777bdad190bc6f0623d9d57b9d967e5fe\n",
      "  Stored in directory: c:\\users\\gvoff\\appdata\\local\\pip\\cache\\wheels\\c1\\3e\\c3\\e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
      "Successfully built docx\n",
      "Installing collected packages: docx\n",
      "Successfully installed docx-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cef44c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Ensure NLTK dependencies are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa7de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gvoff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizations found: ['ACQUISITION', 'ARE', 'ATL', 'Amul Model Consumer Founded', 'Ananth Technologies Ananth Technologies', 'BHEL', 'BMW', 'BTL', 'Bharatiya Janata Party', 'Business', 'CAGR', 'CATEGORIES', 'CITATIONS', 'CUSTOMERS', 'Category Wise Growth', 'Category³ Dairy Bakery', 'Centum Electronics', 'Consultant', 'Consumer', 'DESPITE', 'Demonetization', 'Department', 'Digital', 'Digital Economy', 'Digital Payment', 'Digital Payment Service', 'Digital Payment Service Article', 'Disposable Incomes', 'DoS', 'Downfall', 'ELEPHANTS', 'EXPLORE', 'Economic Times', 'FAQ', 'FELT', 'FMCG', 'FMCG Firms Personal', 'FMCG Landscape', 'FY', 'GMV', 'General Mills', 'Godrej Aerospace', 'Gold Gifting', 'HEARD', 'HUL', 'HUL Nestle India', 'Hamburg', 'HandWash', 'Household Care Food', 'IMMEDIATE', 'IMPROVE', 'INDIA', 'INTRODUCE', 'ISRO', 'ISRP', 'ITC Amul Adani Wilmar United Spirits', 'Increasing Urbanisation', 'JAIN Deemed', 'Know', 'LVM3', 'LVM3M4', 'Local', 'MNC', 'MNCs', 'MTAR', 'MTAR Technologies', 'MTR', 'Maggi Juicy Specialz', 'Merc', 'NAL', 'NCPI', 'NCPI Introduction Unified Payments', 'NSIL', 'National Aerospace Laboratories', 'National Payment Corporation', 'National Payments Corporation', 'Nature Valley Granola Bars', 'NewSpace India Limited', 'OC', 'PARTNERSHIPS Companies', 'PCC', 'PRODUCT', 'PayPal', 'Payments Bank', 'Paytm Gold Investments Plan', 'Paytm Payments Bank Ltd', 'PepsiCo', 'PepsiCo Estimate', 'PepsiCo India', 'Personal', 'Potential Contribution', 'RANK', 'RBI', 'RTC', 'Reserve Bank', 'Rise', 'Role', 'SEEN', 'Saffola Life', 'See Exhibit', 'Side', 'SlimLite Cooking Spray', 'Social Media', 'Sunsilk Gang', 'THE', 'Taste of India', 'Tata', 'Traditional', 'Trumps', 'UB', 'UPI', 'US', 'United Spirits', 'University', 'VPA', 'Value', 'Variants', 'Walchandnagar Industries', 'Wellness', 'Wild West']\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Ensure NLTK dependencies are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to load and extract text from multiple documents\n",
    "def load_documents(file_paths):\n",
    "    all_text = []\n",
    "    for file in file_paths:\n",
    "        try:\n",
    "            doc = docx.Document(file)\n",
    "            doc_text = '\\n'.join(para.text for para in doc.paragraphs)\n",
    "            all_text.append(doc_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    return '\\n'.join(all_text)\n",
    "\n",
    "# Function to extract only named entities of type ORGANIZATION\n",
    "def get_organizations(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    chunked = ne_chunk(pos_tags)\n",
    "    \n",
    "    # Use a set to store unique organizations\n",
    "    organizations = {\n",
    "        ' '.join(c[0] for c in chunk.leaves())\n",
    "        for chunk in chunked if isinstance(chunk, Tree) and chunk.label() == 'ORGANIZATION'\n",
    "    }\n",
    "    \n",
    "    return sorted(organizations)  # Sorted for readability\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # List of document file paths\n",
    "    file_paths = ['List of companies and startups contributing to Cha.docx', \n",
    "                  '20172.docx', \n",
    "                  'INDIA’S TOP 25 FMCG COMPANIES.docx']\n",
    "    \n",
    "    # Load and concatenate text from all documents\n",
    "    doc_text = load_documents(file_paths)\n",
    "    \n",
    "    if doc_text:\n",
    "        # Extract only organization entities\n",
    "        organizations = get_organizations(doc_text)\n",
    "        print(\"Organizations found:\", organizations)\n",
    "    else:\n",
    "        print(\"No document content found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d780f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
